{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "engaged-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "front-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run read_c.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bca7f6",
   "metadata": {},
   "source": [
    "## 取得所有檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "destroyed-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得某個階段的所有檔名\n",
    "def get_filename(root_dir, year):\n",
    "    dir = os.listdir(root_dir)\n",
    "\n",
    "    allfile = []\n",
    "    for file in dir:\n",
    "        path = os.path.join(root_dir, file)\n",
    "       \n",
    "        tmp = glob.glob(path + '/*.P' + str(year))\n",
    "        tmp1 = glob.glob(path + '/*.[1-9]' + str(year))\n",
    "        allfile = allfile + tmp + tmp1\n",
    "    \n",
    "    return allfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663c571",
   "metadata": {},
   "source": [
    "## 以 pfile 為主，把 afile 資訊合併進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "imperial-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_afile_to_pfile(a, p):\n",
    "    tmp_factor = []\n",
    "    \n",
    "    for a_stream in a:\n",
    "        station = a_stream.stats.station\n",
    "        cur_axis, factor, instrument = get_factor(a_stream)\n",
    "        \n",
    "        # 檢查 station 有沒有在 pfile dictionary 裡面出現\n",
    "        if (station not in p.keys()) or (cur_axis == 'none'):\n",
    "            continue\n",
    "        #else:\n",
    "        tmp_factor.append(factor)\n",
    "        \n",
    "        # 先取得要存進 pfile 的 data\n",
    "        network = a_stream.stats.network\n",
    "        location = a_stream.stats.location\n",
    "        sampling_rate = a_stream.stats.sampling_rate\n",
    "        starttime = str(a_stream.stats.starttime)\n",
    "        endtime = str(a_stream.stats.endtime)\n",
    "        channel = a_stream.stats.channel\n",
    "       \n",
    "        # 初始化: 讓 pfile dict 一些欄位轉成 list type\n",
    "        if 'network' not in p[station].keys():\n",
    "            p[station]['network'] = list()\n",
    "        if 'location' not in p[station].keys():\n",
    "            p[station]['location'] = list()\n",
    "        if 'factor' not in p[station].keys():\n",
    "            p[station]['factor'] = list()\n",
    "        if 'sampling_rate' not in p[station].keys():\n",
    "            p[station]['sampling_rate'] = list()\n",
    "        if 'starttime' not in p[station].keys():\n",
    "            p[station]['starttime'] = list()\n",
    "        if 'endtime' not in p[station].keys():\n",
    "            p[station]['endtime'] = list()\n",
    "        if 'instrument' not in p[station].keys():\n",
    "            p[station]['instrument'] = list()\n",
    "        if 'datatype' not in p[station].keys():\n",
    "            p[station]['datatype'] = list()\n",
    "\n",
    "        # 加入 pfile 的 dictionary 之中\n",
    "        if channel == 'Ch3' or channel == 'Ch6' or channel == 'Ch9':\n",
    "            flist = tmp_factor.copy()\n",
    "            p[station]['factor'].append(flist)\n",
    "            p[station]['network'].append(network)\n",
    "            p[station]['location'].append(location)\n",
    "            p[station]['sampling_rate'].append(sampling_rate)\n",
    "            p[station]['starttime'].append(starttime)\n",
    "            p[station]['endtime'].append(endtime)\n",
    "            p[station]['instrument'].append(instrument)\n",
    "            \n",
    "            if channel == 'Ch3':\n",
    "                p[station]['datatype'].append('Acceleration')\n",
    "            else:\n",
    "                p[station]['datatype'].append('Velocity')\n",
    "            \n",
    "            tmp_factor.clear()\n",
    "        \n",
    "        # 加入 E, N, Z 進 dictionary 之中, \n",
    "        if cur_axis == 'z':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'Z' not in p[station].keys():\n",
    "                p[station]['Z'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['Z'] = np.vstack([p[station]['Z'], a_stream.data])\n",
    "        elif cur_axis == 'n':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'N' not in p[station].keys():\n",
    "                p[station]['N'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['N'] = np.vstack([p[station]['N'], a_stream.data])\n",
    "        elif cur_axis == 'e':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'E' not in p[station].keys():\n",
    "                p[station]['E'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['E'] = np.vstack([p[station]['E'], a_stream.data])\n",
    "       \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b65522",
   "metadata": {},
   "source": [
    "## 把波型資料轉成 list，才能存進 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "optical-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arr_to_list(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            for sub_key in p[k].keys():\n",
    "                if sub_key == 'E' or sub_key == 'N' or sub_key == 'Z': \n",
    "                    p[k][sub_key] = p[k][sub_key].tolist()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246af68",
   "metadata": {},
   "source": [
    "## 把沒有波型資料的測站刪除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "miniature-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_no_data(p):\n",
    "    del_sta = []\n",
    "    for k in p.keys():\n",
    "            try:\n",
    "                if ('E' not in p[k].keys()) or ('N' not in p[k].keys()) or ('Z' not in p[k].keys()):\n",
    "                    del_sta.append(k)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    for todel in del_sta:\n",
    "        del p[todel]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bab81",
   "metadata": {},
   "source": [
    "## 因為單一測站有多組資料，複製到時、震度、PGA、PGV 數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "300df346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_values(p):\n",
    "    year = int(p['ori_time'][:4])\n",
    "    \n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # 有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "              \n",
    "                # 複製 p & s_arrival time, intensity\n",
    "                p_time = p[k]['p_arrival_time']\n",
    "                s_time = p[k]['s_arrival_time']\n",
    "                S_avail = p[k]['S']\n",
    "                intensity = p[k]['intensity']\n",
    "                pga = p[k]['pga']\n",
    "                \n",
    "                # ============================================= #\n",
    "                #     舊制的 intensirty, pga, pgv 都為 False     #\n",
    "                # ============================================= #\n",
    "                # check intensity, pga, pgv\n",
    "                is_intensity = False\n",
    "                is_pga = False\n",
    "                is_pgv = False\n",
    "                \n",
    "                # ============================================= #\n",
    "                #           檢查 intensity, pga, pgv            #\n",
    "                # ============================================= #\n",
    "                # 只有 2020 之後的有 pgv\n",
    "                if year >= 2020:\n",
    "                    pgv = p[k]['pgv']\n",
    "                    del p[k]['pgv']\n",
    "                    \n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    if intensity == -1:\n",
    "                        is_intensity = False\n",
    "                    else:\n",
    "                        is_intensity = True\n",
    "                    if pga == -1 or pga == 0:\n",
    "                        is_pga = False\n",
    "                    else:\n",
    "                        is_pga = True\n",
    "                    if pgv == -1 or pgv == 0:\n",
    "                        is_pgv = False\n",
    "                    else:\n",
    "                        is_pgv = True\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(pgv)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                # 2019 以前都沒有 PGV，先用 nan 代替\n",
    "                else:\n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(-1)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                        \n",
    "                # ============================================= #\n",
    "                #          刪除原始欄位，改用 list 取代           #\n",
    "                # ============================================= #\n",
    "                del p[k]['p_arrival_time']\n",
    "                del p[k]['s_arrival_time']\n",
    "                del p[k]['intensity']\n",
    "                del p[k]['pga']\n",
    "                del p[k]['S']\n",
    "                \n",
    "                p[k]['p_arrival_time'] = []\n",
    "                p[k]['s_arrival_time'] = []\n",
    "                p[k]['intensity'] = []\n",
    "                p[k]['instrument_isWork'] = []\n",
    "                p[k]['pga'] = []\n",
    "                p[k]['isIntensity'] = []\n",
    "                p[k]['isPga'] = []\n",
    "                p[k]['isStime'] = []\n",
    "                \n",
    "                # ============================================= #\n",
    "                #       複製原始資料裡面的一些 attributes         #\n",
    "                # ============================================= #\n",
    "                for i in range(n_data):\n",
    "                    p[k]['p_arrival_time'].append(p_time)\n",
    "                    p[k]['s_arrival_time'].append(s_time)\n",
    "                    p[k]['intensity'].append(intensity)\n",
    "                    p[k]['instrument_isWork'].append(True)\n",
    "                    p[k]['pga'].append(pga)\n",
    "                    p[k]['isIntensity'].append(is_intensity)\n",
    "                    p[k]['isPga'].append(is_pga)\n",
    "                    p[k]['isStime'].append(S_avail)\n",
    "                \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025b371",
   "metadata": {},
   "source": [
    "## 整合多項數據的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59bb06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_attributes(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # ============================================= #\n",
    "                #              取得數據的有效性 list             #\n",
    "                # ============================================= #\n",
    "                instrument = p[k]['instrument_isWork']\n",
    "                intensity = p[k]['isIntensity']\n",
    "                pga = p[k]['isPga']\n",
    "                pgv = p[k]['isPgv']\n",
    "                s = p[k]['isStime']\n",
    "               \n",
    "                del p[k]['instrument_isWork']\n",
    "                del p[k]['isIntensity']\n",
    "                del p[k]['isPga']\n",
    "                del p[k]['isPgv']\n",
    "                del p[k]['isStime']\n",
    "                \n",
    "                avail = {}\n",
    "                avail['instrument'] = instrument\n",
    "                avail['intensity'] = intensity\n",
    "                avail['pga'] = pga\n",
    "                avail['pgv'] = pgv\n",
    "                avail['Stime'] = s\n",
    "                p[k]['DataAvailable'] = avail\n",
    "        except:\n",
    "            pass\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eab55",
   "metadata": {},
   "source": [
    "## 把同測站不同組數據整合在同個 key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec7cf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_json(p, files, year):\n",
    "    # 新增 primary key 欄位\n",
    "    month = int(files[:2]) - 12\n",
    "    month = str(month) if month/10==1 else '0'+str(month)\n",
    "    version = files[-7]\n",
    "    p['event'] = year[-2:] + str(month) + files[2:8] + version\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            if 'location' in p[k].keys():\n",
    "                # 看有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "                output_dict = {}\n",
    "                p[k]['numberOfData'] = n_data\n",
    "\n",
    "                # 逐一拿出資料\n",
    "                for n in range(n_data):\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict['network'] = p[k]['network'][n]\n",
    "                    tmp_dict['location'] = p[k]['location'][n]\n",
    "                    tmp_dict['factor'] = p[k]['factor'][n]\n",
    "                    tmp_dict['sampling_rate'] = p[k]['sampling_rate'][n]\n",
    "                    tmp_dict['starttime'] = p[k]['starttime'][n]\n",
    "                    tmp_dict['endtime'] = p[k]['endtime'][n]\n",
    "                    tmp_dict['instrument'] = p[k]['instrument'][n]\n",
    "                    tmp_dict['datatype'] = p[k]['datatype'][n]\n",
    "                    if n_data > 1:\n",
    "                        tmp_dict['Z'], tmp_dict['N'], tmp_dict['E'] = p[k]['Z'][n], p[k]['N'][n], p[k]['E'][n]\n",
    "                    else:\n",
    "                        tmp_dict['Z'], tmp_dict['N'], tmp_dict['E'] = p[k]['Z'], p[k]['N'], p[k]['E']\n",
    "                    tmp_dict['pga'], tmp_dict['pgv'] = p[k]['pga'][n], p[k]['pgv'][n]\n",
    "                    tmp_dict['p_arrival_time'], tmp_dict['s_arrival_time'] = p[k]['p_arrival_time'][n], p[k]['s_arrival_time'][n]\n",
    "                    tmp_dict['intensity'] = p[k]['intensity'][n]\n",
    "                    tmp_dict['DataAvailable'] = {}\n",
    "                    for key in p[k]['DataAvailable'].keys():\n",
    "                        tmp_dict['DataAvailable'][key] = p[k]['DataAvailable'][key][n]\n",
    "\n",
    "                    # 新增到依順序建立的新 key\n",
    "                    output_dict[str(n)] = tmp_dict\n",
    "\n",
    "                # 刪除要改掉的 keys\n",
    "                del p[k]['network'], p[k]['location'], p[k]['factor'], p[k]['sampling_rate'], p[k]['starttime']\n",
    "                del p[k]['endtime'], p[k]['instrument'], p[k]['datatype'], p[k]['Z'], p[k]['N'], p[k]['E']\n",
    "                del p[k]['pga'], p[k]['pgv'], p[k]['p_arrival_time'], p[k]['s_arrival_time']\n",
    "                del p[k]['intensity'], p[k]['DataAvailable']\n",
    "\n",
    "                # 把改好的加進原始資料中\n",
    "                for modify_k in output_dict.keys():\n",
    "                    p[k][modify_k] = output_dict[modify_k]\n",
    "\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-withdrawal",
   "metadata": {},
   "source": [
    "# 從這裡開始執行以產生 json 檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "naked-vision",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                   | 0/564 [00:27<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# target: 2020, 2019, 2018, 2017, 2016, 2015, 2014\n",
    "all_year = ['2020', '2019', '2018', '2017', '2016', '2015', '2014']\n",
    "for year in all_year:\n",
    "    sub_fname = year[2:]\n",
    "    base_path = '/mnt/nas6/new_CWB_data/CWB_data/' + year + '/felt'\n",
    "    save_base_path = os.path.join('/mnt/nas6/CWBSN', year)\n",
    "    files = get_filename(base_path, year[-2:])\n",
    "\n",
    "    # mkdir\n",
    "    if not os.path.exists(save_base_path):\n",
    "        print('making directory...', save_base_path)\n",
    "        os.mkdir(save_base_path)\n",
    "        \n",
    "    # generate dataset\n",
    "    gen(files, save_base_path, sub_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "willing-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(files, save_base_path, sub_fname):\n",
    "    for f in tqdm(range(len(files))):\n",
    "        try:\n",
    "            filename = files[f][:-4]\n",
    "            pfile = files[f]\n",
    "            num_p = files[f][-3]\n",
    "            if num_p == 'P':\n",
    "                num_p = '0'\n",
    "\n",
    "            # output json file\n",
    "            json_file = files[f][-12:-4] + '(' + num_p + ')' + '.json'\n",
    "            save_path = os.path.join(save_base_path, json_file)\n",
    "\n",
    "            # if repeated, don't save as json\n",
    "            #if os.path.exists(save_path):\n",
    "                #continue\n",
    "\n",
    "            a = unpackAfile(filename + '.A' + sub_fname)\n",
    "            if sub_fname == '20':\n",
    "                p = unpackPfile_2020(pfile)  # 2020 ~\n",
    "            else:\n",
    "                p = unpackPfile(pfile)   # ~ 2019 \n",
    "\n",
    "            # 把 afile 資訊加入 pfile's dictionary\n",
    "            p = append_afile_to_pfile(a, p)\n",
    "\n",
    "            # 把 pfile 裡面的 ndarray 轉換成 list 才能存進 dictionary\n",
    "            p = convert_arr_to_list(p)    \n",
    "\n",
    "            # 把沒有加速度資料的測站刪掉\n",
    "            p = delete_no_data(p)\n",
    "\n",
    "            # 改一些欄位\n",
    "            p = copy_values(p)\n",
    "\n",
    "            # 整合一些欄位\n",
    "            p = concat_attributes(p)\n",
    "\n",
    "            # 最後修改 json 欄位\n",
    "            p = modify_json(p, json_file, sub_fname)\n",
    "          \n",
    "            # write\n",
    "            with open(save_path, 'w') as file:\n",
    "                json.dump(p, file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e212fd",
   "metadata": {},
   "source": [
    "## 重跑遺失的單一事件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d045b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"/mnt/nas6/new_CWB_data/CWB_data/2019/felt/09/21250951.P19\"\n",
    "save_base_path = os.path.join('/mnt/nas6/CWBDatasets', '2019')\n",
    "sub_fname='19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fec6767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = files[:-4]\n",
    "pfile = files\n",
    "num_p = files[-3]\n",
    "if num_p == 'P':\n",
    "    num_p = '0'\n",
    "\n",
    "# output json file\n",
    "save_path = os.path.join(save_base_path, files[-12:-4]) + '(' + num_p + ')' + '.json'\n",
    "\n",
    "a = unpackAfile(filename + '.A' + sub_fname)\n",
    "p = unpackPfile(pfile)   # ~ 2019 \n",
    "#p = unpackPfile_2020(pfile)  # 2020 ~\n",
    "\n",
    "# 把 afile 資訊加入 pfile's dictionary\n",
    "p = append_afile_to_pfile(a, p)\n",
    "\n",
    "# 把 pfile 裡面的 ndarray 轉換成 list 才能存進 dictionary\n",
    "p = convert_arr_to_list(p)    \n",
    "\n",
    "# 把沒有加速度資料的測站刪掉\n",
    "p = delete_no_data(p)\n",
    "\n",
    "# 改一些欄位\n",
    "p = copy_values(p)\n",
    "\n",
    "# 整合一些欄位\n",
    "p = concat_attributes(p)\n",
    "\n",
    "# write\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(p, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5c2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Earthworm",
   "language": "python",
   "name": "earthworm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
