{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaged-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "front-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run read_c.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bca7f6",
   "metadata": {},
   "source": [
    "## 取得所有檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得某個階段的所有檔名\n",
    "def get_filename(root_dir, year):\n",
    "    dir = os.listdir(root_dir)\n",
    "\n",
    "    allfile = []\n",
    "    for file in dir:\n",
    "        path = os.path.join(root_dir, file)\n",
    "       \n",
    "        tmp = glob.glob(path + '/*.P' + str(year))\n",
    "        tmp1 = glob.glob(path + '/*.[1-9]' + str(year))\n",
    "        allfile = allfile + tmp + tmp1\n",
    "    \n",
    "    return allfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663c571",
   "metadata": {},
   "source": [
    "## 以 pfile 為主，把 afile 資訊合併進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imperial-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_afile_to_pfile(a, p):\n",
    "    tmp_factor = []\n",
    "    \n",
    "    for a_stream in a:\n",
    "        station = a_stream.stats.station\n",
    "        cur_axis, factor, instrument = get_factor(a_stream)\n",
    "        \n",
    "        # 檢查 station 有沒有在 pfile dictionary 裡面出現\n",
    "        if (station not in p.keys()) or (cur_axis == 'none'):\n",
    "            continue\n",
    "        #else:\n",
    "        tmp_factor.append(factor)\n",
    "        \n",
    "        # 先取得要存進 pfile 的 data\n",
    "        network = a_stream.stats.network\n",
    "        location = a_stream.stats.location\n",
    "        sampling_rate = a_stream.stats.sampling_rate\n",
    "        starttime = str(a_stream.stats.starttime)\n",
    "        endtime = str(a_stream.stats.endtime)\n",
    "        channel = a_stream.stats.channel\n",
    "       \n",
    "        # 初始化: 讓 pfile dict 一些欄位轉成 list type\n",
    "        if 'network' not in p[station].keys():\n",
    "            p[station]['network'] = list()\n",
    "        if 'location' not in p[station].keys():\n",
    "            p[station]['location'] = list()\n",
    "        if 'factor' not in p[station].keys():\n",
    "            p[station]['factor'] = list()\n",
    "        if 'sampling_rate' not in p[station].keys():\n",
    "            p[station]['sampling_rate'] = list()\n",
    "        if 'starttime' not in p[station].keys():\n",
    "            p[station]['starttime'] = list()\n",
    "        if 'endtime' not in p[station].keys():\n",
    "            p[station]['endtime'] = list()\n",
    "        if 'instrument' not in p[station].keys():\n",
    "            p[station]['instrument'] = list()\n",
    "        if 'datatype' not in p[station].keys():\n",
    "            p[station]['datatype'] = list()\n",
    "\n",
    "        # 加入 pfile 的 dictionary 之中\n",
    "        if channel == 'Ch3' or channel == 'Ch6' or channel == 'Ch9':\n",
    "            flist = tmp_factor.copy()\n",
    "            p[station]['factor'].append(flist)\n",
    "            p[station]['network'].append(network)\n",
    "            p[station]['location'].append(location)\n",
    "            p[station]['sampling_rate'].append(sampling_rate)\n",
    "            p[station]['starttime'].append(starttime)\n",
    "            p[station]['endtime'].append(endtime)\n",
    "            p[station]['instrument'].append(instrument)\n",
    "            \n",
    "            if channel == 'Ch3':\n",
    "                p[station]['datatype'].append('Acceleration')\n",
    "            else:\n",
    "                p[station]['datatype'].append('Velocity')\n",
    "            \n",
    "            tmp_factor.clear()\n",
    "        \n",
    "        # 加入 E, N, Z 進 dictionary 之中, \n",
    "        if cur_axis == 'z':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'Z' not in p[station].keys():\n",
    "                p[station]['Z'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['Z'] = np.vstack([p[station]['Z'], a_stream.data])\n",
    "        elif cur_axis == 'n':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'N' not in p[station].keys():\n",
    "                p[station]['N'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['N'] = np.vstack([p[station]['N'], a_stream.data])\n",
    "        elif cur_axis == 'e':\n",
    "            # check if ground acceleraiont is exist\n",
    "            if 'E' not in p[station].keys():\n",
    "                p[station]['E'] = a_stream.data\n",
    "            else:\n",
    "                p[station]['E'] = np.vstack([p[station]['E'], a_stream.data])\n",
    "       \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b65522",
   "metadata": {},
   "source": [
    "## 把波型資料轉成 list，才能存進 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "optical-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arr_to_list(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            for sub_key in p[k].keys():\n",
    "                if sub_key == 'E' or sub_key == 'N' or sub_key == 'Z': \n",
    "                    p[k][sub_key] = p[k][sub_key].tolist()\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246af68",
   "metadata": {},
   "source": [
    "## 把沒有波型資料的測站刪除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "miniature-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_no_data(p):\n",
    "    del_sta = []\n",
    "    for k in p.keys():\n",
    "            try:\n",
    "                if ('E' not in p[k].keys()) or ('N' not in p[k].keys()) or ('Z' not in p[k].keys()):\n",
    "                    del_sta.append(k)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    for todel in del_sta:\n",
    "        del p[todel]\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bab81",
   "metadata": {},
   "source": [
    "## 因為單一測站有多組資料，複製到時、震度、PGA、PGV 數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "300df346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_values(p):\n",
    "    year = int(p['ori_time'][:4])\n",
    "    \n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # 有幾組資料\n",
    "                n_data = len(p[k]['location'])\n",
    "              \n",
    "                # 複製 p & s_arrival time, intensity\n",
    "                p_time = p[k]['p_arrival_time']\n",
    "                s_time = p[k]['s_arrival_time']\n",
    "                S_avail = p[k]['S']\n",
    "                intensity = p[k]['intensity']\n",
    "                pga = p[k]['pga']\n",
    "                \n",
    "                # ============================================= #\n",
    "                #     舊制的 intensirty, pga, pgv 都為 False     #\n",
    "                # ============================================= #\n",
    "                # check intensity, pga, pgv\n",
    "                is_intensity = False\n",
    "                is_pga = False\n",
    "                is_pgv = False\n",
    "                \n",
    "                # ============================================= #\n",
    "                #           檢查 intensity, pga, pgv            #\n",
    "                # ============================================= #\n",
    "                # 只有 2020 之後的有 pgv\n",
    "                if year >= 2020:\n",
    "                    pgv = p[k]['pgv']\n",
    "                    del p[k]['pgv']\n",
    "                    \n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    if intensity == -1:\n",
    "                        is_intensity = False\n",
    "                    else:\n",
    "                        is_intensity = True\n",
    "                    if pga == -1 or pga == 0:\n",
    "                        is_pga = False\n",
    "                    else:\n",
    "                        is_pga = True\n",
    "                    if pgv == -1 or pgv == 0:\n",
    "                        is_pgv = False\n",
    "                    else:\n",
    "                        is_pgv = True\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(pgv)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                # 2019 以前都沒有 PGV，先用 nan 代替\n",
    "                else:\n",
    "                    p[k]['pgv'] = []\n",
    "                    p[k]['isPgv'] = []\n",
    "                    \n",
    "                    for i in range(n_data):\n",
    "                        p[k]['pgv'].append(-1)\n",
    "                        p[k]['isPgv'].append(is_pgv)\n",
    "                        \n",
    "                # ============================================= #\n",
    "                #          刪除原始欄位，改用 list 取代           #\n",
    "                # ============================================= #\n",
    "                del p[k]['p_arrival_time']\n",
    "                del p[k]['s_arrival_time']\n",
    "                del p[k]['intensity']\n",
    "                del p[k]['pga']\n",
    "                del p[k]['S']\n",
    "                \n",
    "                p[k]['p_arrival_time'] = []\n",
    "                p[k]['s_arrival_time'] = []\n",
    "                p[k]['intensity'] = []\n",
    "                p[k]['instrument_isWork'] = []\n",
    "                p[k]['pga'] = []\n",
    "                p[k]['isIntensity'] = []\n",
    "                p[k]['isPga'] = []\n",
    "                p[k]['isStime'] = []\n",
    "                \n",
    "                # ============================================= #\n",
    "                #       複製原始資料裡面的一些 attributes         #\n",
    "                # ============================================= #\n",
    "                for i in range(n_data):\n",
    "                    p[k]['p_arrival_time'].append(p_time)\n",
    "                    p[k]['s_arrival_time'].append(s_time)\n",
    "                    p[k]['intensity'].append(intensity)\n",
    "                    p[k]['instrument_isWork'].append(True)\n",
    "                    p[k]['pga'].append(pga)\n",
    "                    p[k]['isIntensity'].append(is_intensity)\n",
    "                    p[k]['isPga'].append(is_pga)\n",
    "                    p[k]['isStime'].append(S_avail)\n",
    "                \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025b371",
   "metadata": {},
   "source": [
    "## 整合多項數據的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bb06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_attributes(p):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            # 篩選 key = station \n",
    "            if 'location' in p[k].keys():\n",
    "                # ============================================= #\n",
    "                #              取得數據的有效性 list             #\n",
    "                # ============================================= #\n",
    "                instrument = p[k]['instrument_isWork']\n",
    "                intensity = p[k]['isIntensity']\n",
    "                pga = p[k]['isPga']\n",
    "                pgv = p[k]['isPgv']\n",
    "                s = p[k]['isStime']\n",
    "               \n",
    "                del p[k]['instrument_isWork']\n",
    "                del p[k]['isIntensity']\n",
    "                del p[k]['isPga']\n",
    "                del p[k]['isPgv']\n",
    "                del p[k]['isStime']\n",
    "                \n",
    "                avail = {}\n",
    "                avail['instrument'] = instrument\n",
    "                avail['intensity'] = intensity\n",
    "                avail['pga'] = pga\n",
    "                avail['pgv'] = pgv\n",
    "                avail['Stime'] = s\n",
    "                p[k]['DataAvailable'] = avail\n",
    "        except:\n",
    "            pass\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-withdrawal",
   "metadata": {},
   "source": [
    "# 從這裡開始執行以產生 json 檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "naked-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish: 2020, 2019, 2018, 2017, 2016, 2015, 2014\n",
    "year = '2020'\n",
    "sub_fname = year[2:]\n",
    "base_path = '/mnt/nas6/new_CWB_data/CWB_data/' + year + '/felt'\n",
    "save_base_path = os.path.join('/mnt/nas6/CWBDatasets', year)\n",
    "# save_base_path = os.path.join('/home/weiwei/server-docker/docker-home', year)\n",
    "files = get_filename(base_path, year[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "offshore-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making directory... /mnt/nas6/CWBDatasets/2020\n"
     ]
    }
   ],
   "source": [
    "# mkdir\n",
    "if not os.path.exists(save_base_path):\n",
    "    print('making directory...', save_base_path)\n",
    "    os.mkdir(save_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-chance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                               | 6/509 [02:12<3:14:21, 23.18s/it]"
     ]
    }
   ],
   "source": [
    "for f in tqdm(range(len(files))):\n",
    "    try:\n",
    "        filename = files[f][:-4]\n",
    "        pfile = files[f]\n",
    "        num_p = files[f][-3]\n",
    "        if num_p == 'P':\n",
    "            num_p = '0'\n",
    "            \n",
    "        # output json file\n",
    "        save_path = os.path.join(save_base_path, files[f][-12:-4]) + '(' + num_p + ')' + '.json'\n",
    "\n",
    "        # if repeated, don't save as json\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "            \n",
    "        a = unpackAfile(filename + '.A' + sub_fname)\n",
    "#         p = unpackPfile(pfile)   # ~ 2019 \n",
    "        p = unpackPfile_2020(pfile)  # 2020 ~\n",
    "        \n",
    "        # 把 afile 資訊加入 pfile's dictionary\n",
    "        p = append_afile_to_pfile(a, p)\n",
    "\n",
    "        # 把 pfile 裡面的 ndarray 轉換成 list 才能存進 dictionary\n",
    "        p = convert_arr_to_list(p)    \n",
    "\n",
    "        # 把沒有加速度資料的測站刪掉\n",
    "        p = delete_no_data(p)\n",
    "        \n",
    "        # 改一些欄位\n",
    "        p = copy_values(p)\n",
    "        \n",
    "        # 整合一些欄位\n",
    "        p = concat_attributes(p)\n",
    "        \n",
    "        # write\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(p, file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d045b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Earthworm",
   "language": "python",
   "name": "earthworm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
